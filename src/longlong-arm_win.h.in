/*
   Copyright 1991, 1992, 1993, 1994, 1996, 1997, 1999, 2000, 2001, 2002, 2003,
   2004, 2005 Free Software Foundation, Inc.

   Copyright 2009, 2015, 2016 William Hart
   Copyright 2011 Fredrik Johansson
   Copyright 2023 Albin Ahlb√§ck

   This file is free software; you can redistribute it and/or modify
   it under the terms of the GNU Lesser General Public License as published by
   the Free Software Foundation; either version 2.1 of the License, or (at your
   option) any later version.

   This file is distributed in the hope that it will be useful, but
   WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
   or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public
   License for more details.

   You should have received a copy of the GNU Lesser General Public License
   along with this file; see the file COPYING.LIB.  If not, write to
   the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston,
   MA 02110-1301, USA.
*/

/*
   N.B: This file has been adapted from code found in GMP 4.2.1.
*/

#ifndef FLINT_LONGLONG_H
#define FLINT_LONGLONG_H

#ifdef __cplusplus
extern "C" {
#endif

#include <intrin.h>
#include <stdlib.h>

#define FLINT_KNOW_STRONG_ORDER 0

#if GMP_LIMB_BITS == 32
# define flint_clz _CountLeadingZeros
static inline int flint_ctz(ulong n)
{
    unsigned long index;
    _BitScanReverse(&index, n);
    return index;
}
# define byte_swap(n) do { n = _byteswap_ulong(n); } while (0)
#else
# define flint_clz _CountLeadingZeros64
static inline int flint_ctz(ulong n)
{
    unsigned long index;
    _BitScanReverse64(&index, n);
    return index;
}
# define byte_swap(n) do { n = _byteswap_uint64(n); } while (0)
#endif

#define add_ssaaaa(s1, s0, a1, a0, b1, b0)\
  do {                                    \
    unsigned char carry;                  \
    (s0) = (a0) + (b0);                   \
    carry = (ulong) (s0) < (ulong) (a0);  \
    (s1) = (a1) + (b1) + carry;           \
  } while (0)

#define add_sssaaaaaa(sh, sm, sl, ah, am, al, bh, bm, bl)     \
  do {                                                        \
    mp_limb_t __t, __u;                                       \
    add_ssaaaa(__t, sl, (mp_limb_t) 0, al, (mp_limb_t) 0, bl);\
    add_ssaaaa(__u, sm, (mp_limb_t) 0, am, (mp_limb_t) 0, bm);\
    add_ssaaaa(sh, sm, ah + bh, sm, __u, __t);                \
  } while (0)

#define add_ssssaaaaaaaa(s3, s2, s1, s0, a3, a2, a1, a0, b3, b2, b1, b0)      \
  do {                                                                        \
    mp_limb_t __tt;                                                           \
    add_sssaaaaaa(__tt, s1, s0, (mp_limb_t) 0, a1, a0, (mp_limb_t) 0, b1, b0);\
    add_ssaaaa(s3, s2, a3, a2, b3, b2);                                       \
    add_ssaaaa(s3, s2, s3, s2, (mp_limb_t) 0, __tt);                          \
  } while (0)

#define sub_ddmmss(s1, s0, a1, a0, b1, b0)\
  do {                                    \
    unsigned char carry;                  \
    (s0) = (a0) - (b0);                   \
    carry = (ulong) (a0) < (ulong) (b0);  \
    (s1) = (a1) - (b1) - carry;           \
  } while (0)

#define sub_dddmmmsss(dh, dm, dl, mh, mm, ml, sh, sm, sl)       \
  do {                                                          \
    mp_limb_t __t, __u;                                         \
    sub_ddmmss(__t, dl, (mp_limb_t) 0, ml, (mp_limb_t) 0, sl);  \
    sub_ddmmss(__u, dm, (mp_limb_t) 0, mm, (mp_limb_t) 0, sm);  \
    sub_ddmmss(dh, dm, (mh) - (sh), dm, -__u, -__t);            \
  } while (0)

#if GMP_LIMB_BITS == 32
# define umul_ppmm(w1, w0, u, v)  \
  do {                            \
    unsigned long long int res;   \
    res = _arm_umull(u, v);       \
    (w0) = (ulong) res;           \
    (w1) = (ulong) (res >> 32);   \
  } while (0)

# define smul_ppmm(w1, w0, u, v)  \
  do {                            \
    long long int res;            \
    res = _arm_smull(u, v);       \
    (w0) = (slong) res;           \
    (w1) = (slong) (res >> 32);   \
  } while (0)
#else
# define umul_ppmm(w1, w0, u, v)  \
  do {                            \
    (w0) = (u) * (v);             \
    (w1) = __umulh(u, v);         \
  } while (0)

# define smul_ppmm(w1, w0, u, v)                        \
  do {                                                  \
    mp_limb_t __w1;                                     \
    mp_limb_t __xm0 = (u), __xm1 = (v);                 \
    umul_ppmm (__w1, w0, __xm0, __xm1);                 \
    (w1) = __w1 - (-(__xm0 >> (FLINT_BITS-1)) & __xm1)  \
        - (-(__xm1 >> (FLINT_BITS-1)) & __xm0);         \
  } while (0)
#endif

#define __ll_B ((mp_limb_t) 1 << (GMP_LIMB_BITS / 2))
#define __ll_lowpart(t) ((mp_limb_t) (t) & (__ll_B - 1))
#define __ll_highpart(t) ((mp_limb_t) (t) >> (GMP_LIMB_BITS / 2))
#define __highbit (~(mp_limb_t)0 ^ ((~(mp_limb_t)0) >> 1))

#define udiv_qrnnd_int(q, r, n1, n0, d)                 \
  do {                                                  \
    mp_limb_t __d1, __d0, __q1, __q0, __r1, __r0, __m;  \
                                                        \
    FLINT_ASSERT ((d) != 0);                            \
    FLINT_ASSERT ((n1) < (d));                          \
                                                        \
    __d1 = __ll_highpart (d);                           \
    __d0 = __ll_lowpart (d);                            \
                                                        \
    __q1 = (n1) / __d1;                                 \
    __r1 = (n1) - __q1 * __d1;                          \
    __m = __q1 * __d0;                                  \
    __r1 = __r1 * __ll_B | __ll_highpart (n0);          \
    if (__r1 < __m)                                     \
    {                                                   \
      __q1--, __r1 += (d);                              \
      if (__r1 >= (d)) /* i.e. we didn't get carry when adding to __r1 */ \
        if (__r1 < __m)                                 \
          __q1--, __r1 += (d);                          \
    }                                                   \
    __r1 -= __m;                                        \
                                                        \
    __q0 = __r1 / __d1;                                 \
    __r0 = __r1  - __q0 * __d1;                         \
    __m = __q0 * __d0;                                  \
    __r0 = __r0 * __ll_B | __ll_lowpart (n0);           \
    if (__r0 < __m)                                     \
    {                                                   \
      __q0--, __r0 += (d);                              \
      if (__r0 >= (d))                                  \
        if (__r0 < __m)                                 \
          __q0--, __r0 += (d);                          \
    }                                                   \
    __r0 -= __m;                                        \
                                                        \
    (q) = __q1 * __ll_B | __q0;                         \
    (r) = __r0;                                         \
  } while (0)

#define udiv_qrnnd(q, r, n1, n0, d)             \
  do {                                          \
    mp_limb_t __norm = flint_clz(d);            \
    if (__norm)                                 \
    {                                           \
      udiv_qrnnd_int((q), (r), ((n1) << __norm) + ((n0) >> (GMP_LIMB_BITS - __norm)), (n0) << __norm, (d) << __norm); \
      (r) = ((mp_limb_t) (r) >> __norm);        \
    }                                           \
    else                                        \
      udiv_qrnnd_int((q), (r), (n1), (n0), (d));\
  } while (0)

#define sdiv_qrnnd(q, r, n1, n0, d)         \
  do {                                      \
    mp_limb_t __n1, __n0, __d;              \
    mp_limb_t __q, __r;                     \
    unsigned int __sgn_n = 0, __sgn_d = 0;  \
    if ((n1) & __highbit)                   \
    {                                       \
       __n0 = -(n0);                        \
       __n1 = ~(n1) + (__n0 == 0);          \
       __sgn_n = ~__sgn_n;                  \
    } else                                  \
    {                                       \
      __n0 = (n0);                          \
      __n1 = (n1);                          \
    }                                       \
    if ((d) & __highbit)                    \
    {                                       \
        __d = -(d);                         \
        __sgn_d = ~__sgn_d;                 \
    } else                                  \
    {                                       \
        __d = (d);                          \
    }                                       \
    udiv_qrnnd(__q, __r, __n1, __n0, __d);  \
    q = (__sgn_n == __sgn_d) ? __q : -__q;  \
    r = (__sgn_n == 0) ? __r : -__r;        \
  } while (0)

#define udiv_qrnnd_preinv(q, r, nh, nl, d, di)               \
  do {                                                       \
    mp_limb_t _n2, _n10, _nmask, _nadj, _q1;                 \
    mp_limb_t _xh, _xl;                                      \
    _n2 = (nh);                                              \
    _n10 = (nl);                                             \
    _nmask = (mp_limb_signed_t) (_n10) >> (FLINT_BITS - 1);  \
    _nadj = _n10 + (_nmask & (d));                           \
    umul_ppmm (_xh, _xl, di, _n2 - _nmask);                  \
    add_ssaaaa (_xh, _xl, _xh, _xl, _n2, _nadj);             \
    _q1 = ~_xh;                                              \
    umul_ppmm (_xh, _xl, _q1, d);                            \
    add_ssaaaa (_xh, _xl, _xh, _xl, nh, nl);                 \
    _xh -= (d);                 /* xh = 0 or -1 */           \
    (r) = _xl + ((d) & _xh);                                 \
    (q) = _xh - _q1;                                         \
  } while (0)

#ifdef __cplusplus
}
#endif

#endif
