#define FLINT_KNOW_STRONG_ORDER 1

#define add_ssssaaaaaaaa(s3, s2, s1, s0, a3, a2, a1, a0, b3, b2, b1, b0)  \
  __asm__ ("addq %11,%q3\n\tadcq %9,%q2\n\tadcq %7,%q1\n\tadcq %5,%q0"    \
       : "=r" (s3), "=&r" (s2), "=&r" (s1), "=&r" (s0)                    \
       : "0"  ((ulong)(a3)), "rme" ((ulong)(b3)),                 \
         "1"  ((ulong)(a2)), "rme" ((ulong)(b2)),                 \
         "2"  ((ulong)(a1)), "rme" ((ulong)(b1)),                 \
         "3"  ((ulong)(a0)), "rme" ((ulong)(b0)))                 \

#define add_sssaaaaaa(sh, sm, sl, ah, am, al, bh, bm, bl)  \
  __asm__ ("addq %8,%q2\n\tadcq %6,%q1\n\tadcq %4,%q0"     \
       : "=r" (sh), "=&r" (sm), "=&r" (sl)                  \
       : "0"  ((ulong)(ah)), "rme" ((ulong)(bh)),  \
         "1"  ((ulong)(am)), "rme" ((ulong)(bm)),  \
         "2"  ((ulong)(al)), "rme" ((ulong)(bl)))  \

#define sub_dddmmmsss(dh, dm, dl, mh, mm, ml, sh, sm, sl)  \
  __asm__ ("subq %8,%q2\n\tsbbq %6,%q1\n\tsbbq %4,%q0"     \
       : "=r" (dh), "=&r" (dm), "=&r" (dl)                  \
       : "0"  ((ulong)(mh)), "rme" ((ulong)(sh)),  \
         "1"  ((ulong)(mm)), "rme" ((ulong)(sm)),  \
         "2"  ((ulong)(ml)), "rme" ((ulong)(sl)))  \

#define add_ssaaaa(sh, sl, ah, al, bh, bl)                 \
  __asm__ ("addq %5,%q1\n\tadcq %3,%q0"                    \
       : "=r" (sh), "=&r" (sl)                             \
       : "0"  ((ulong)(ah)), "rme" ((ulong)(bh)),  \
         "%1" ((ulong)(al)), "rme" ((ulong)(bl)))

#define sub_ddmmss(sh, sl, ah, al, bh, bl)                \
  __asm__ ("subq %5,%q1\n\tsbbq %3,%q0"                   \
       : "=r" (sh), "=&r" (sl)                            \
       : "0" ((ulong)(ah)), "rme" ((ulong)(bh)),  \
         "1" ((ulong)(al)), "rme" ((ulong)(bl)))

#define umul_ppmm(w1, w0, u, v)                         \
  __asm__ ("mulq %3"                                    \
       : "=a" (w0), "=d" (w1)                           \
       : "%0" ((ulong)(u)), "rm" ((ulong)(v)))

#define smul_ppmm(w1, w0, u, v)                         \
  __asm__ ("imulq %3"                                   \
       : "=a" (w0), "=d" (w1)                           \
       : "%0" ((ulong)(u)), "rm" ((ulong)(v)))

#define udiv_qrnnd(q, r, n1, n0, dx)                                            \
  __asm__ volatile ("divq %4"                                                            \
       : "=a" (q), "=d" (r)                                                     \
       : "0" ((ulong)(n0)), "1" ((ulong)(n1)), "rm" ((ulong)(dx)))

#define sdiv_qrnnd(q, r, n1, n0, dx)                                            \
  __asm__ volatile ("idivq %4"                                                           \
       : "=a" (q), "=d" (r)                                                     \
       : "0" ((ulong)(n0)), "1" ((ulong)(n1)), "rm" ((ulong)(dx)))

#define udiv_qrnnd_preinv(q, r, nh, nl, d, di)               \
  do {                                                       \
    ulong _n2, _n10, _nmask, _nadj, _q1;                 \
    ulong _xh, _xl;                                      \
    _n2 = (nh);                                              \
    _n10 = (nl);                                             \
    _nmask = (mp_limb_signed_t) (_n10) >> (FLINT_BITS - 1);  \
    _nadj = _n10 + (_nmask & (d));                           \
    umul_ppmm (_xh, _xl, di, _n2 - _nmask);                  \
    add_ssaaaa (_xh, _xl, _xh, _xl, _n2, _nadj);             \
    _q1 = ~_xh;                                              \
    umul_ppmm (_xh, _xl, _q1, d);                            \
    add_ssaaaa (_xh, _xl, _xh, _xl, nh, nl);                 \
    _xh -= (d);                 /* xh = 0 or -1 */           \
    (r) = _xl + ((d) & _xh);                                 \
    (q) = _xh - _q1;                                         \
  } while (0)

/* bsrq destination must be a 64-bit register, hence ulong for __cbtmp. */
#define count_leading_zeros(count, x)                                 \
  do {                                                                \
    ulong __cbtmp;                                                \
    FLINT_ASSERT ((x) != 0);                                          \
    __asm__ ("bsrq %1,%0" : "=r" (__cbtmp) : "rm" ((ulong)(x)));  \
    (count) = __cbtmp ^ (ulong) 63;                               \
  } while (0)

/* bsfq destination must be a 64-bit register, "%q0" forces this in case
   count is only an int. */
#define count_trailing_zeros(count, x)                               \
  do {                                                               \
    FLINT_ASSERT ((x) != 0);                                         \
    __asm__ ("bsfq %1,%q0" : "=r" (count) : "rm" ((ulong)(x)));  \
  } while (0)

#define byte_swap(x)                                                 \
  do {                                                               \
    __asm__("bswapq %q0" : "=r"(x) : "0"(x));                         \
  } while (0)

/* rec_word_tab[i] = div(2^19 - 3*2^8, 2^8 + i) */
static const int rec_word_tab[256] = {
   2045, 2037, 2029, 2021, 2013, 2005, 1998, 1990, 1983, 1975, 1968, 1960, 1953, 1946, 1938, 1931,
   1924, 1917, 1910, 1903, 1896, 1889, 1883, 1876, 1869, 1863, 1856, 1849, 1843, 1836, 1830, 1824,
   1817, 1811, 1805, 1799, 1792, 1786, 1780, 1774, 1768, 1762, 1756, 1750, 1745, 1739, 1733, 1727,
   1722, 1716, 1710, 1705, 1699, 1694, 1688, 1683, 1677, 1672, 1667, 1661, 1656, 1651, 1646, 1641,
   1636, 1630, 1625, 1620, 1615, 1610, 1605, 1600, 1596, 1591, 1586, 1581, 1576, 1572, 1567, 1562,
   1558, 1553, 1548, 1544, 1539, 1535, 1530, 1526, 1521, 1517, 1513, 1508, 1504, 1500, 1495, 1491,
   1487, 1483, 1478, 1474, 1470, 1466, 1462, 1458, 1454, 1450, 1446, 1442, 1438, 1434, 1430, 1426,
   1422, 1418, 1414, 1411, 1407, 1403, 1399, 1396, 1392, 1388, 1384, 1381, 1377, 1374, 1370, 1366,
   1363, 1359, 1356, 1352, 1349, 1345, 1342, 1338, 1335, 1332, 1328, 1325, 1322, 1318, 1315, 1312,
   1308, 1305, 1302, 1299, 1295, 1292, 1289, 1286, 1283, 1280, 1276, 1273, 1270, 1267, 1264, 1261,
   1258, 1255, 1252, 1249, 1246, 1243, 1240, 1237, 1234, 1231, 1228, 1226, 1223, 1220, 1217, 1214,
   1211, 1209, 1206, 1203, 1200, 1197, 1195, 1192, 1189, 1187, 1184, 1181, 1179, 1176, 1173, 1171,
   1168, 1165, 1163, 1160, 1158, 1155, 1153, 1150, 1148, 1145, 1143, 1140, 1138, 1135, 1133, 1130,
   1128, 1125, 1123, 1121, 1118, 1116, 1113, 1111, 1109, 1106, 1104, 1102, 1099, 1097, 1095, 1092,
   1090, 1088, 1086, 1083, 1081, 1079, 1077, 1074, 1072, 1070, 1068, 1066, 1064, 1061, 1059, 1057,
   1055, 1053, 1051, 1049, 1047, 1044, 1042, 1040, 1038, 1036, 1034, 1032, 1030, 1028, 1026, 1024
};

#define invert_limb(dinv, d)                                      \
   do {                                                           \
      ulong _v0, _v2, _d40, _e, _m0;                          \
      FLINT_ASSERT(((d) & (UWORD(1)<<(GMP_LIMB_BITS - 1))) != 0); \
      _d40 = ((d) >> 24) + 1;                                     \
      _v0 = rec_word_tab[((d) >> 55) & 0xFF];                     \
      _v0 = (_v0 << 11) - ((_v0*_v0*_d40) >> 40) - 1;             \
      _v2 = ((_v0*((((ulong) 1) << 60) - _v0*_d40)) >> 47);   \
      _v2 += (_v0 << 13);                                         \
      _e = -_v2*((d) >> 1);                                       \
      _m0 = -((d) & (ulong) 1);                               \
      _e -= ((_v2 - (_v2 >> 1)) & _m0);                           \
      umul_ppmm(_v0, _d40, _v2, _e);                              \
      _v2 = (_v2 << 31) + (_v0 >> 1);                             \
      umul_ppmm(_v0, _d40, _v2, (d));                             \
      add_ssaaaa(_v0, _d40, _v0, _d40, (ulong) 0, (d));       \
      (dinv) = _v2 - (_v0 + (d));                                 \
   } while (0)
